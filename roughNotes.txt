idempotent : These are functions that produce the same output for a given input no matter
how many times you run it. These are used in data pipelines incase a rerun is required.

We have copied the starter files to set up the local infrastructure, this will save us time from
doing unnecessary work.


Common docker commands:
docker-compose -f docker-compose-LocalExecutor.yml up -d  :
docker-compose A Docker tool to define and manage multi-container applications using a YAML configuration file.
-f option tell docker-compose that we're passing a file(docker-compose-LocalExecutor.yml)
up : this will build, create, start and attach container. Incase it's not build, it build it first
-d : this ensures the continer runs in the background, so you get your terminal control back after spinning it up.


pgcli connection: 
pgcli -h localhost -p 5432 -U airflow -d airflow
-h is host IP address
-p port number
-U user name
-d database name


Loading data into table:
\copy retail.user_purchase(invoice_number,
stock_code,detail,quantity,
invoice_date,unit_price,customer_id,country) 
FROM 'D:/Data Engineering/Project/customer-behaviour/customer-behaviour/data/retail/OnlineRetail.csv' 
DELIMITER ','  CSV HEADER;



Okay, so, the above command is pretty self explainatory, but here's a catch see the "\" in the start?
That's used to upload data onto a remote server. Incase your postgres is running locally, it's not required.
But here's the thing, the postgres server that we're using is actually running on docker, so complely
isolated from the local machine, so it is like uploading it to a remote server and can't be treated
like a local server and miss the "\". Commands starting with "\" run on the client side and not the
server side and so, it's able to access the file that the server can't.
One more thing, use "\copy" and not "\COPY" when using pgcli, this is because pgcli does not expect
it to be in caps. While postgres itself is case insensitive, pgcli is not(in some cases like this).
This won't happen if we use the standard CLI, psql, it handles both the cases well.

Your text editor could say that you have introduced a few errors when writing a DAG file, but what's
important is that you write the file based on the Airflow instance that running and not the "pip" module
you've installed, because DAG file is like a config file to Airflow. Unlike a regular python file,
python will not execute the file, it's Airflow that will execute it. Also, wait for about 5 mins or so
for your DAG to appear in Airflow.


Creating a connection with AWS for Airflow:
log in to Airflow UI >> Admin >> connections >> create
Name the connection, conn type: Amazon web services / Amazon s3/ S3 (depending on airflow version)
Now, this connection will be used to instanstiate the hook in Airflow `S3Hook('connection name')`

Always add the region when connecting to a S3 bucket, the buckets are assigned a region(always) so
make sure your connection URL does not say 'global'
Add this in extra:
Example(Change region according to your bucket)
{
   "region_name": "ap-southeast-2"
}