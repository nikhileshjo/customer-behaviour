idempotent : These are functions that produce the same output for a given input no matter
how many times you run it. These are used in data pipelines incase a rerun is required.

We have copied the starter files to set up the local infrastructure, this will save us time from
doing unnecessary work.


Common docker commands:
docker-compose -f docker-compose-LocalExecutor.yml up -d  :
docker-compose A Docker tool to define and manage multi-container applications using a YAML configuration file.
-f option tell docker-compose that we're passing a file(docker-compose-LocalExecutor.yml)
up : this will build, create, start and attach container. Incase it's not build, it build it first
-d : this ensures the continer runs in the background, so you get your terminal control back after spinning it up.


pgcli connection: 
pgcli -h localhost -p 5432 -U airflow -d airflow
-h is host IP address
-p port number
-U user name
-d database name


Loading data into table:
\copy retail.user_purchase(invoice_number,
stock_code,detail,quantity,
invoice_date,unit_price,customer_id,country) 
FROM 'D:/Data Engineering/Project/customer-behaviour/customer-behaviour/data/retail/OnlineRetail.csv' 
DELIMITER ','  CSV HEADER;



Okay, so, the above command is pretty self explainatory, but here's a catch see the "\" in the start?
That's used to upload data onto a remote server. Incase your postgres is running locally, it's not required.
But here's the thing, the postgres server that we're using is actually running on docker, so complely
isolated from the local machine, so it is like uploading it to a remote server and can't be treated
like a local server and miss the "\". Commands starting with "\" run on the client side and not the
server side and so, it's able to access the file that the server can't.
One more thing, use "\copy" and not "\COPY" when using pgcli, this is because pgcli does not expect
it to be in caps. While postgres itself is case insensitive, pgcli is not(in some cases like this).
This won't happen if we use the standard CLI, psql, it handles both the cases well.

Your text editor could say that you have introduced a few errors when writing a DAG file, but what's
important is that you write the file based on the Airflow instance that running and not the "pip" module
you've installed, because DAG file is like a config file to Airflow. Unlike a regular python file,
python will not execute the file, it's Airflow that will execute it. Also, wait for about 5 mins or so
for your DAG to appear in Airflow.


Creating a connection with AWS for Airflow:
log in to Airflow UI >> Admin >> connections >> create
Name the connection, conn type: Amazon web services / Amazon s3/ S3 (depending on airflow version)
Now, this connection will be used to instanstiate the hook in Airflow `S3Hook('connection name')`

Always add the region when connecting to a S3 bucket, the buckets are assigned a region(always) so
make sure your connection URL does not say 'global'
Add this in extra:
Example(Change region according to your bucket)
{
   "region_name": "ap-southeast-2"
}

AWS EMR:
When creating an AWS EMR instance, we can add a Bootstrap file from S3 bucket. This is a ssh file
which is run the first thing after cluster creation. It's useful for configuring the server better.


Connecting to Redshift
pgcli -h <your-redshift-host> -U <your-user-name> -p 5439 -d <yourredshift-database>

When connecting to Redshift, there's a conflict in encoding between Psycopg and AWS
https://github.com/psycopg/psycopg2/issues/1539
So, the fix is as suggested in the link above, if you've installed python in the default location,
you need to add the below code to the file:
_encodings.py

Add key value pair to the dictionary _py_codecs
"UNICODE": "utf-8"

and you'll be able to login to AWS


Understanding a few terms:

wait_for_downstream : It waits for the immediate downstream task of the previously ran instance to finish. For example:
This is my task order A>>B>>C>>D

if the task ran yesterday ie on 21 Dec, then today's task A from 22 Dec, will wait for B from 21 Dec to finish before it starts.
The task B will wait for task C from yesterday and so on.
Note: if wait_for_downstream is set True, then depends_on_past is forced to True.


depends_on_past : It checks if the previously ran instance of the same task was completed.
So again, A>>B>>C>>D if this ran yesterday and is running again today, let's say B, it will check
if B from yesterday(21 Dec) was successful and only if true, it will run B today(22 Dec).

Airflow xcom:
Before you learn about xcom, you need to know a little about task instance:
https://airflow.apache.org/docs/apache-airflow/stable/public-airflow-interface.html#task-instances
Basically, it's a pointer to the task, like 'self' in python class.
Now, about xcom. This is a way of passing data between tasks. This does not allow exchange of enoumous amounts of data like dataframes.
But it can allow things like metadata, or a value returned (same as what we're doing)



One thing to learn about the EMR Bootstrap is that as long as the bootstarp are in the list, they all are executed.
The radio buttons are there to manipulate them, you can remove one of the files using it.